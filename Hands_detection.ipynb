{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinaEmili/Sign_language/blob/main/Hands_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kCtae26tygt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea8c4bd-019a-43bc-b15b-022a19177014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload() # here upload 'chrome-cookies.txt'"
      ],
      "metadata": {
        "id": "GWysCrzjjEbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yt_dlp"
      ],
      "metadata": {
        "id": "SFrlTkPRGrMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import requests\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nibhY2zmQX79"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp"
      ],
      "metadata": {
        "id": "_U9_-84-TOh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the JSON data\n",
        "js_file = pd.read_json(\"/content/drive/MyDrive/NLP/WLASL_v0.3.json\")\n",
        "folder = \"/content/drive/MyDrive/NLP/dataset/\"\n",
        "\n",
        "training_folder = folder + \"train/\"\n",
        "test_folder = folder + \"test/\"\n",
        "training_video = training_folder + \"video/\"\n",
        "test_video = test_folder + \"video/\"\n",
        "training_images = training_folder + \"images/\"\n",
        "test_images = test_folder + \"images/\"\n",
        "os.makedirs(training_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "os.makedirs(training_video, exist_ok=True)\n",
        "os.makedirs(test_video, exist_ok=True)\n",
        "os.makedirs(training_images, exist_ok=True)\n",
        "os.makedirs(test_images, exist_ok=True)"
      ],
      "metadata": {
        "id": "p7P2WNJh7rIi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sources:\n",
        "# {'asl5200',     youtube.com\n",
        "# 'aslbrick',     mp4\n",
        "# 'asldeafined',  mp4\n",
        "# 'asllex',       youtu.be\n",
        "# 'aslpro',       swf\n",
        "# 'aslsearch',    mp4\n",
        "# 'aslsignbank',  mp4\n",
        "# 'aslu',         youtube.com\n",
        "# 'elementalasl', mov (like mp4 maybe?)\n",
        "# 'handspeak',    mp4\n",
        "# 'lillybauer',   youtube.com\n",
        "# 'nabboud',      youtube.com\n",
        "# 'northtexas',   youtube.com\n",
        "# 'scott',        youtube.com\n",
        "# 'signingsavvy', mp4\n",
        "# 'signschool',   mp4\n",
        "# 'spreadthesign',mp4\n",
        "# 'startasl',     mp4\n",
        "# 'valencia-asl'} youtube.com"
      ],
      "metadata": {
        "id": "CpS3isnki9Ut"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download youtube videos\n",
        "\n",
        "youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']\n",
        "not_downloaded = 0\n",
        "tot_youtube_videos = 0\n",
        "downloaded = []\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    url = js_file['instances'][i][j]['url']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source in youtube_videos:\n",
        "      tot_youtube_videos += 1\n",
        "      output_path = training_video if split == 'train' else test_video\n",
        "      dest_path = os.path.join(output_path, filename)\n",
        "      if 'youtu.be/' in url:\n",
        "          continue\n",
        "\n",
        "      if os.path.exists(dest_path):\n",
        "        downloaded.append(video_id)\n",
        "        continue\n",
        "\n",
        "      ydl_opts = {\n",
        "        'outtmpl': dest_path,\n",
        "        'format': 'bestvideo+bestaudio/best',\n",
        "        'merge_output_format': 'mp4',\n",
        "        'quiet': True,\n",
        "        'sleep_interval_requests': 2,\n",
        "        'max_sleep_interval': 5,\n",
        "        'cookies': 'chrome-cookies.txt'\n",
        "        }\n",
        "      try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "          ydl.download([url])\n",
        "          downloaded.append(video_id)\n",
        "          #print(video_id, 'DOWNLOADED')\n",
        "      except:\n",
        "        #print(video_id)\n",
        "        not_downloaded += 1\n",
        "        continue"
      ],
      "metadata": {
        "id": "cioQr7UCGNnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download non-youtube videos (mp4 or mov)\n",
        "# 9734 non-youtube videos\n",
        "\n",
        "youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    url = js_file['instances'][i][j]['url']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source not in youtube_videos:\n",
        "\n",
        "      output_path = training_video if split == 'train' else test_video\n",
        "      dest_path = os.path.join(output_path, filename)\n",
        "\n",
        "      # remove already downloaded but damaged videos\n",
        "      if os.path.exists(dest_path):\n",
        "        with open(dest_path, 'rb') as f:\n",
        "                header = f.read()\n",
        "                if b'<html' in header.lower() or b'403 Forbidden' in header:\n",
        "                  os.remove(dest_path)\n",
        "\n",
        "      try:\n",
        "        resp = requests.get(url, stream=True)\n",
        "        # do not download damaged videos\n",
        "        if resp.status_code != 200:\n",
        "                    continue\n",
        "        # save downloaded videos\n",
        "        with open(dest_path, mode='wb') as f:\n",
        "            f.write(resp.content)\n",
        "      except:\n",
        "        continue"
      ],
      "metadata": {
        "id": "_0cpCM5ec2MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapipe\n",
        "# hand module\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "# pose module\n",
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ],
      "metadata": {
        "id": "-d98bdFoTv35"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization pose estimations\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  pose_landmarks_list = detection_result.pose_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "    # Draw the pose landmarks.\n",
        "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    pose_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      pose_landmarks_proto,\n",
        "      solutions.pose.POSE_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
        "  return annotated_image"
      ],
      "metadata": {
        "id": "bGzggb6qp-dq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pose detection\n",
        "\n",
        "import mediapipe as mp # STEP 1: Import the necessary modules.\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "sample = training_video + \"adjective_01066.mp4\"\n",
        "i, j = 1603, 3\n",
        "cap = cv2.VideoCapture(sample)\n",
        "frame_end = js_file['instances'][int(i)][int(j)]['frame_end']\n",
        "frame_start = js_file['instances'][int(i)][int(j)]['frame_start']\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "step_frame = (frame_end - frame_start + 1) // 8\n",
        "curr_frame = 0\n",
        "printed_frames = 0\n",
        "\n",
        "# STEP 2: Create an PoseLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    output_segmentation_masks=True)\n",
        "detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "  if curr_frame < frame_start:\n",
        "    curr_frame += 1\n",
        "  if frame_end != -1 and curr_frame > frame_end:\n",
        "    break\n",
        "  if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 8:\n",
        "    data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # STEP 3: Load the input image.\n",
        "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "    # STEP 4: Detect hand landmarks from the input image.\n",
        "    detection_result = detector.detect(image)\n",
        "    # STEP 5: Process the classification result. In this case, visualize it.\n",
        "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "    #cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "    landmarks = detection_result.pose_landmarks[0]\n",
        "    for idx, landmark in enumerate(landmarks):\n",
        "      x = landmark.x # Normalized x coordinate [0.0, 1.0]\n",
        "      y = landmark.y # Normalized x coordinate [0.0, 1.0]\n",
        "      z = landmark.z # Relative depth\n",
        "      visibility = landmark.visibility # Confidence score of the landmark visibility\n",
        "    printed_frames += 1\n",
        "  curr_frame += 1\n",
        "\n",
        "# details about the landmarks https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker"
      ],
      "metadata": {
        "id": "Ck8NIQbfeKIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp # STEP 1: Import the necessary modules.\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    url = js_file['instances'][i][j]['url']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    frame_end = js_file['instances'][i][j]['frame_end']\n",
        "    frame_start = js_file['instances'][i][j]['frame_start']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source not in youtube_videos:\n",
        "      path = training_video if split == 'train' else test_video\n",
        "      sample = path + filename\n",
        "      cap = cv2.VideoCapture(sample)\n",
        "      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "      step_frame = (frame_end - frame_start + 1) // 8\n",
        "      curr_frame = 0\n",
        "      printed_frames = 0\n",
        "\n",
        "      # STEP 2: Create an PoseLandmarker object.\n",
        "      base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "      options = vision.PoseLandmarkerOptions(\n",
        "          base_options=base_options,\n",
        "          output_segmentation_masks=True)\n",
        "      detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "      while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "          break\n",
        "        if curr_frame < frame_start:\n",
        "          curr_frame += 1\n",
        "        if frame_end != -1 and curr_frame > frame_end:\n",
        "          break\n",
        "        if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 8:\n",
        "          data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          # STEP 3: Load the input image.\n",
        "          image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "          # STEP 4: Detect hand landmarks from the input image.\n",
        "          detection_result = detector.detect(image)\n",
        "          # STEP 5: Process the classification result. In this case, visualize it.\n",
        "          annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "          image_path = training_images if split == 'train' else test_images\n",
        "          pose_folder = image_path + \"pose/\"\n",
        "          os.makedirs(pose_folder, exist_ok=True)\n",
        "          output_path = pose_folder + f\"{word}_{video_id}_{printed_frames}.jpg\"\n",
        "          cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "          printed_frames += 1\n",
        "        curr_frame += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwN5GuLp1qIV",
        "outputId": "615ff509-e013-4ca7-db71-26f3550a02e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "glosses:  10%|â–ˆ         | 203/2000 [1:00:12<7:24:12, 14.83s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization hand estimation\n",
        "\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result.hand_landmarks\n",
        "  handedness_list = detection_result.handedness\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "    handedness = handedness_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Get the top left corner of the detected hand's bounding box.\n",
        "    height, width, _ = annotated_image.shape\n",
        "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
        "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
        "    text_x = int(min(x_coordinates) * width)\n",
        "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "    # Draw handedness (left or right hand) on the image.\n",
        "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
        "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "  return annotated_image"
      ],
      "metadata": {
        "id": "fLCXnnlEymvH"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hand detection\n",
        "import mediapipe as mp # STEP 1: Import the necessary modules.\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "sample = training_video + \"adjective_01066.mp4\"\n",
        "i, j = 1603, 3\n",
        "cap = cv2.VideoCapture(sample)\n",
        "frame_end = js_file['instances'][int(i)][int(j)]['frame_end']\n",
        "frame_start = js_file['instances'][int(i)][int(j)]['frame_start']\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "step_frame = (frame_end - frame_start + 1) // 16\n",
        "curr_frame = 0\n",
        "printed_frames = 0\n",
        "\n",
        "# STEP 2: Create an HandLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                       num_hands=2)\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "  if curr_frame < frame_start:\n",
        "    curr_frame += 1\n",
        "  if frame_end != -1 and curr_frame > frame_end:\n",
        "    break\n",
        "  if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 32:\n",
        "    data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # STEP 3: Load the input image.\n",
        "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "    # STEP 4: Detect hand landmarks from the input image.\n",
        "    detection_result = detector.detect(image)\n",
        "    # STEP 5: Process the classification result. In this case, visualize it.\n",
        "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "    landmarks = detection_result.hand_landmarks\n",
        "    if landmarks != []:\n",
        "      # print only images in which the hand(s) has(ve) been detected\n",
        "      cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "      for idx, hand_landmarks in enumerate(landmarks):\n",
        "        x = [landmark.x for landmark in hand_landmarks] # Normalized x coordinate [0.0, 1.0]\n",
        "        y = [landmark.y for landmark in hand_landmarks] # Normalized x coordinate [0.0, 1.0]\n",
        "        z = [landmark.z for landmark in hand_landmarks] # Relative depth\n",
        "        visibility = [landmark.visibility for landmark in hand_landmarks] # Confidence score of the landmark visibility\n",
        "\n",
        "    printed_frames += 1\n",
        "  curr_frame += 1\n",
        "\n",
        "# details about the landmarks https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker"
      ],
      "metadata": {
        "id": "Fv7g6nyjyEir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}