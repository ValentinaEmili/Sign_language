{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinaEmili/Sign_language/blob/main/ASL_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code from\n",
        "\n",
        "https://github.com/AvishakeAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model/blob/main/RealTimeSignLanguageDetection.ipynb"
      ],
      "metadata": {
        "id": "z4864mwIJuZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeY0eM3iPdbW"
      },
      "outputs": [],
      "source": [
        "# mount google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LSTM\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "syDlsXWzPjzp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the JSON data\n",
        "js_file = pd.read_json(\"/content/drive/MyDrive/NLP/WLASL_v0.3.json\")\n",
        "folder = \"/content/drive/MyDrive/NLP/dataset/\"\n",
        "\n",
        "training_folder = folder + \"train/\"\n",
        "test_folder = folder + \"test/\"\n",
        "training_video = training_folder + \"video/\"\n",
        "test_video = test_folder + \"video/\"\n",
        "training_images = training_folder + \"images/\"\n",
        "test_images = test_folder + \"images/\"\n",
        "\n",
        "#youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']"
      ],
      "metadata": {
        "id": "FSJFrrsePo3B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data"
      ],
      "metadata": {
        "id": "ciyIxRuPHyYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gloss = set()\n",
        "for image in os.listdir(training_images):\n",
        "  word, _ = image.split(\"_\")\n",
        "  gloss.add(word)\n",
        "for image in os.listdir(test_images):\n",
        "  word, _ = image.split(\"_\")\n",
        "  gloss.add(word)\n",
        "gloss = list(gloss)\n",
        "label_map = {label: num for num, label in enumerate(gloss)}\n",
        "X_train, y_train, X_test, y_test = [], [], [], []\n",
        "\n",
        "for image in os.listdir(training_images):\n",
        "  np_array = np.load(os.path.join(training_images, image))\n",
        "  label, _ = image.split(\"_\")\n",
        "  X_train.append(np_array)\n",
        "  y_train.append(label_map[label])\n",
        "\n",
        "for image in os.listdir(test_images):\n",
        "  np_array = np.load(os.path.join(test_images, image))\n",
        "  label, _ = image.split(\"_\")\n",
        "  X_test.append(np_array)\n",
        "  y_test.append(label_map[label])\n",
        "\n",
        "num_classes = len(gloss)\n",
        "\n",
        "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "y_train = F.one_hot(y_train, num_classes=num_classes).int()\n",
        "y_test = F.one_hot(y_test, num_classes=num_classes).int()"
      ],
      "metadata": {
        "id": "dHivL3dkfqO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train LSTM Neural Network"
      ],
      "metadata": {
        "id": "YVI0dkQFHtgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"/content/drive/MyDrive/NLP/Logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "2VgnD53cIOQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation='softmax'))"
      ],
      "metadata": {
        "id": "yoaCgb2DI4kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "i0Mvuo1xJFUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=100, callbacks=[tb_callback])"
      ],
      "metadata": {
        "id": "Ly6TVWnHJHgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.predict(X_test)\n",
        "print(\"predicted\", actions[np.argmax(res[4])])\n",
        "print(\"ground truth\", actions[np.argmax(y_test[4])])"
      ],
      "metadata": {
        "id": "1QRaLP2MJQ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save weights"
      ],
      "metadata": {
        "id": "uanWtjbSJdj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./model.h5')\n",
        "model.save_weights('./model_weights.h5');"
      ],
      "metadata": {
        "id": "fkyganlmJgVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#del model"
      ],
      "metadata": {
        "id": "Dth1dYKBJiTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}