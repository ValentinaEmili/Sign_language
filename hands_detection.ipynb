{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinaEmili/Sign_language/blob/main/hands_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.5"
      ],
      "metadata": {
        "id": "MfmNEfnxd61H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F0UDLxVxTq5O",
        "outputId": "cf8bbc88-932e-47da-8cdc-56124eca2150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import os"
      ],
      "metadata": {
        "id": "sMy2rkRieg8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the JSON data\n",
        "js_file = pd.read_json(\"/content/drive/MyDrive/NLP/WLASL_v0.3.json\")\n",
        "folder = \"/content/drive/MyDrive/NLP/dataset/\"\n",
        "\n",
        "training_folder = folder + \"train/\"\n",
        "validation_folder = folder + \"val/\"\n",
        "test_folder = folder + \"test/\"\n",
        "\n",
        "training_video = training_folder + \"video/\"\n",
        "validation_video = validation_folder + \"video/\"\n",
        "test_video = test_folder + \"video/\"\n",
        "\n",
        "training_images = training_folder + \"images/\"\n",
        "validation_images = validation_folder + \"images/\"\n",
        "test_images = test_folder + \"images/\"\n",
        "\n",
        "youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']"
      ],
      "metadata": {
        "id": "96ibUf7qedT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Holistic model can detect pose, hands and face features.\n",
        "-  pose: 33 keypoints, we will save the landmarks (x, y, z, visibility)\n",
        "- left hand: 21 keypoints, we will save the landmarks (x, y, z)\n",
        "- right hand: 21 keypoints, we will save the landmarks (x, y, z)\n",
        "- face (not used): 468 keypoints\n"
      ],
      "metadata": {
        "id": "KvOzjDclQoWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# holistic model\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    frame_end = js_file['instances'][i][j]['frame_end']\n",
        "    frame_start = js_file['instances'][i][j]['frame_start']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    source_path = training_video if split == 'train' else validation_video if split == 'val' else test_video\n",
        "    dest_path = training_images if split == 'train' else validation_images if split == 'val' else test_images\n",
        "    os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "    # skip videos with broken url\n",
        "    if filename not in os.listdir(source_path):\n",
        "      continue\n",
        "\n",
        "    if source not in youtube_videos and dest_file not in os.listdir(dest_path):\n",
        "      dest_file = f\"{word}_{video_id}.npy\"\n",
        "\n",
        "      cap = cv2.VideoCapture(os.path.join(source_path, filename))\n",
        "\n",
        "      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "      all_frame_features = []\n",
        "      curr_frame = 0\n",
        "\n",
        "      with mp_holistic.Holistic(\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "        while cap.isOpened():\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "            break\n",
        "\n",
        "          if curr_frame < frame_start:\n",
        "            curr_frame += 1\n",
        "            continue\n",
        "\n",
        "          if curr_frame > frame_end:\n",
        "             break\n",
        "          frame_features = []\n",
        "\n",
        "          # to improve performance, optionally mark the image as not writable to pass by inference\n",
        "          frame.flags.writeable = False\n",
        "          image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          results = holistic.process(image)\n",
        "\n",
        "          # save landmarks\n",
        "          if results.pose_landmarks:\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "              frame_features.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "          else:\n",
        "            frame_features.extend([0.0]*132) # 33 points, each with 4 values (x, y, z, visibility)\n",
        "\n",
        "          if results.left_hand_landmarks:\n",
        "            for landmark in results.left_hand_landmarks.landmark:\n",
        "              frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "          else:\n",
        "            frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "\n",
        "          if results.right_hand_landmarks:\n",
        "            for landmark in results.right_hand_landmarks.landmark:\n",
        "              frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "          else:\n",
        "            frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "          if sum(frame_features) > 0:\n",
        "            all_frame_features.append(frame_features) # for the whole video\n",
        "\n",
        "          curr_frame += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        np.save(os.path.join(dest_path, dest_file), np.array(all_frame_features))"
      ],
      "metadata": {
        "id": "dPXqU9b2u4UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without youtube videos:\n",
        "- training set: 6537 videos\n",
        "- validation set: 1787 videos\n",
        "- test set: 1254 videos\n",
        "\n",
        "Training gloss: 1980 words (still to remove words in test and val that are not in training set)\n",
        "\n",
        "With youtube videos:\n",
        "- training set: 8760 videos\n",
        "- validation set: 2493 videos\n",
        "- test set: 2060 videos\n",
        "\n",
        "Training gloss: 1999 words. they're missing 'boots' and 'wash face' from all the three sets"
      ],
      "metadata": {
        "id": "JLkihPO4xaKF"
      }
    }
  ]
}