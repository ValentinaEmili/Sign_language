{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinaEmili/Sign_language/blob/main/hands_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.5"
      ],
      "metadata": {
        "id": "MfmNEfnxd61H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORYdPmelSaYw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# hand module\n",
        "if not os.path.exists(\"hand_landmarker.task\"):\n",
        "  !wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "# pose module\n",
        "if not os.path.exists(\"pose_landmarker.task\"):\n",
        "  !wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F0UDLxVxTq5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import os"
      ],
      "metadata": {
        "id": "sMy2rkRieg8Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the JSON data\n",
        "js_file = pd.read_json(\"/content/drive/MyDrive/NLP/WLASL_v0.3.json\")\n",
        "folder = \"/content/drive/MyDrive/NLP/dataset/\"\n",
        "\n",
        "training_folder = folder + \"train/\"\n",
        "test_folder = folder + \"test/\"\n",
        "training_video = training_folder + \"video/\"\n",
        "test_video = test_folder + \"video/\"\n",
        "training_images = training_folder + \"images/\"\n",
        "test_images = test_folder + \"images/\"\n",
        "\n",
        "youtube_videos = ['asl5200', 'asllex', 'aslu', 'lillybauer', 'nabboud', 'northtexas', 'scott', 'valencia-asl']"
      ],
      "metadata": {
        "id": "96ibUf7qedT5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "splitted = pd.read_csv(\"/content/drive/MyDrive/NLP/splitted.csv\")\n",
        "splitted['train'] = splitted['train'].apply(ast.literal_eval)\n",
        "splitted['test'] = splitted['test'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "RgHGiZrOuP8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Holistic model can detect pose, hands and face features.\n",
        "-  pose: 33 keypoints, we will save the landmarks (x, y, z, visibility)\n",
        "- left hand: 21 keypoints, we will save the landmarks (x, y, z)\n",
        "- right hand: 21 keypoints, we will save the landmarks (x, y, z)\n",
        "- face (not used): 468 keypoints\n"
      ],
      "metadata": {
        "id": "KvOzjDclQoWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# holistic model\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    frame_end = js_file['instances'][i][j]['frame_end']\n",
        "    frame_start = js_file['instances'][i][j]['frame_start']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source not in youtube_videos:\n",
        "      source_path = training_video if split == 'train' else test_video\n",
        "      dest_path = training_images if split == 'train' else test_images\n",
        "      os.makedirs(dest_path, exist_ok=True)\n",
        "      dest_file = f\"landmarks_{word}_{video_id}.npy\"\n",
        "      if dest_file not in dest_path:\n",
        "        cap = cv2.VideoCapture(os.path.join(source_path, filename))\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "        all_frame_features = []\n",
        "        curr_frame = 0\n",
        "\n",
        "        with mp_holistic.Holistic(\n",
        "          min_detection_confidence=0.5,\n",
        "          min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "          while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "              break\n",
        "\n",
        "            if curr_frame < frame_start:\n",
        "              curr_frame += 1\n",
        "              continue\n",
        "\n",
        "            if frame_end != -1 and curr_frame > frame_end:\n",
        "              break\n",
        "            frame_features = []\n",
        "\n",
        "            # to improve performance, optionally mark the image as not writable to pass by inference\n",
        "            frame.flags.writeable = False\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(image)\n",
        "\n",
        "            # save landmarks\n",
        "            if results.pose_landmarks:\n",
        "              for landmark in results.pose_landmarks.landmark:\n",
        "                frame_features.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "            else:\n",
        "              frame_features.extend([0.0]*132) # 33 points, each with 4 values (x, y, z, visibility)\n",
        "\n",
        "            if results.left_hand_landmarks:\n",
        "              for landmark in results.left_hand_landmarks.landmark:\n",
        "                frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "            else:\n",
        "              frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "\n",
        "            if results.right_hand_landmarks:\n",
        "              for landmark in results.right_hand_landmarks.landmark:\n",
        "                frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "            else:\n",
        "              frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "\n",
        "            all_frame_features.append(frame_features) # for the whole video\n",
        "\n",
        "            curr_frame += 1\n",
        "\n",
        "          cap.release()\n",
        "\n",
        "          np.save(os.path.join(dest_path, dest_file), np.array(all_frame_features))"
      ],
      "metadata": {
        "id": "dPXqU9b2u4UP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf30c844-2936-4ec2-8008-9079c15bfcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "glosses:   5%|â–Œ         | 109/2000 [1:31:55<26:26:23, 50.33s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# holistic model sample\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "sample = training_video + \"adjective_01066.mp4\"\n",
        "i, j = 1603, 3\n",
        "cap = cv2.VideoCapture(sample)\n",
        "frame_end = js_file['instances'][int(i)][int(j)]['frame_end']\n",
        "frame_start = js_file['instances'][int(i)][int(j)]['frame_start']\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "step_frame = (frame_end - frame_start + 1) // 16\n",
        "curr_frame = 0\n",
        "printed_frames = 0\n",
        "all_frame_features = []\n",
        "\n",
        "with mp_holistic.Holistic(\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        continue\n",
        "      if curr_frame < frame_start:\n",
        "        curr_frame += 1\n",
        "      if frame_end != -1 and curr_frame > frame_end:\n",
        "        break\n",
        "      frame_features = []\n",
        "      # to improve performance, optionally mark the image as not writable to pass by inference\n",
        "      frame.flags.writeable = False\n",
        "      image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      results = holistic.process(image)\n",
        "\n",
        "      # save landmarks\n",
        "      if results.pose_landmarks:\n",
        "        for landmark in results.pose_landmarks.landmark:\n",
        "          frame_features.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "      else:\n",
        "        frame_features.extend([0.0]*132) # 33 points, each with 4 values (x, y, z, visibility)\n",
        "\n",
        "      if results.left_hand_landmarks:\n",
        "        for landmark in results.left_hand_landmarks.landmark:\n",
        "          frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "      else:\n",
        "        frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "\n",
        "      if results.right_hand_landmarks:\n",
        "        for landmark in results.right_hand_landmarks.landmark:\n",
        "          frame_features.extend([landmark.x, landmark.y, landmark.z])\n",
        "      else:\n",
        "        frame_features.extend([0.0]*63) # points, each with 3 values (x, y, z)\n",
        "\n",
        "      all_frame_features.append(frame_features) # for the whole video\n",
        "#np.save(f\"/content/drive/MyDrive/NLP/dataset/SPLIT/images/landmarks_{video_id}.npy\", all_frame_features)\n"
      ],
      "metadata": {
        "id": "s6Qg3a3FMuMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization pose estimations\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  pose_landmarks_list = detection_result.pose_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected poses to visualize.\n",
        "  for idx in range(len(pose_landmarks_list)):\n",
        "    pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "    # Draw the pose landmarks.\n",
        "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    pose_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      pose_landmarks_proto,\n",
        "      solutions.pose.POSE_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
        "  return annotated_image"
      ],
      "metadata": {
        "id": "gatzRjyfSioo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pose detection\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    url = js_file['instances'][i][j]['url']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    frame_end = js_file['instances'][i][j]['frame_end']\n",
        "    frame_start = js_file['instances'][i][j]['frame_start']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source not in youtube_videos:\n",
        "      image_path = training_images if split == 'train' else test_images\n",
        "      pose_folder = image_path + \"pose/\"\n",
        "      os.makedirs(pose_folder, exist_ok=True)\n",
        "      path = training_video if split == 'train' else test_video\n",
        "      sample = path + filename\n",
        "      cap = cv2.VideoCapture(sample)\n",
        "      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "      step_frame = (frame_end - frame_start + 1) // 8\n",
        "      curr_frame = 0\n",
        "      printed_frames = 0\n",
        "\n",
        "      # STEP 2: Create an PoseLandmarker object.\n",
        "      base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "      options = vision.PoseLandmarkerOptions(\n",
        "          base_options=base_options,\n",
        "          output_segmentation_masks=True)\n",
        "      detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "      while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "          break\n",
        "        if curr_frame < frame_start:\n",
        "          curr_frame += 1\n",
        "        if frame_end != -1 and curr_frame > frame_end:\n",
        "          break\n",
        "        if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 8:\n",
        "          curr_filename = f\"{word}_{video_id}_{printed_frames}.jpg\"\n",
        "          if curr_filename not in os.listdir(pose_folder):\n",
        "            output_path = pose_folder + curr_filename\n",
        "            data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # STEP 3: Load the input image.\n",
        "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "            # STEP 4: Detect hand landmarks from the input image.\n",
        "            detection_result = detector.detect(image)\n",
        "            # STEP 5: Process the classification result. In this case, visualize it.\n",
        "            annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "            cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "            printed_frames += 1\n",
        "        curr_frame += 1\n",
        "\n",
        "# details about the landmarks https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker"
      ],
      "metadata": {
        "id": "NzMbAazISloD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization hand estimation\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  hand_landmarks_list = detection_result.hand_landmarks\n",
        "  handedness_list = detection_result.handedness\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected hands to visualize.\n",
        "  for idx in range(len(hand_landmarks_list)):\n",
        "    hand_landmarks = hand_landmarks_list[idx]\n",
        "    handedness = handedness_list[idx]\n",
        "\n",
        "    # Draw the hand landmarks.\n",
        "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    hand_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "    ])\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "      annotated_image,\n",
        "      hand_landmarks_proto,\n",
        "      solutions.hands.HAND_CONNECTIONS,\n",
        "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "      solutions.drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "    # Get the top left corner of the detected hand's bounding box.\n",
        "    height, width, _ = annotated_image.shape\n",
        "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
        "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
        "    text_x = int(min(x_coordinates) * width)\n",
        "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "    # Draw handedness (left or right hand) on the image.\n",
        "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
        "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "  return annotated_image"
      ],
      "metadata": {
        "id": "yoArif0FSnPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hand detection\n",
        "\n",
        "for i, word in enumerate(tqdm(list(js_file['gloss']), desc='glosses')):\n",
        "  for j, instance in enumerate(js_file['instances'][i]):\n",
        "    video_id = js_file['instances'][i][j]['video_id']\n",
        "    url = js_file['instances'][i][j]['url']\n",
        "    source = js_file['instances'][i][j]['source']\n",
        "    split = js_file['instances'][i][j]['split']\n",
        "    frame_end = js_file['instances'][i][j]['frame_end']\n",
        "    frame_start = js_file['instances'][i][j]['frame_start']\n",
        "    filename = f\"{word}_{video_id}.mp4\"\n",
        "\n",
        "    if source not in youtube_videos:\n",
        "      image_path = training_images if split == 'train' else test_images\n",
        "      hand_folder = image_path + \"hand/\"\n",
        "      os.makedirs(hand_folder, exist_ok=True)\n",
        "      path = training_video if split == 'train' else test_video\n",
        "      sample = path + filename\n",
        "      cap = cv2.VideoCapture(sample)\n",
        "      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "      step_frame = (frame_end - frame_start + 1) // 16\n",
        "      curr_frame = 0\n",
        "      printed_frames = 0\n",
        "\n",
        "      # STEP 2: Create an HandLandmarker object.\n",
        "      base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "      options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                            num_hands=2)\n",
        "      detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "      while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "          break\n",
        "        if curr_frame < frame_start:\n",
        "          curr_frame += 1\n",
        "        if frame_end != -1 and curr_frame > frame_end:\n",
        "          break\n",
        "        if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 16:\n",
        "          curr_filename = f\"{word}_{video_id}_{printed_frames}.jpg\"\n",
        "          if curr_filename not in os.listdir(hand_folder):\n",
        "            output_path = hand_folder + curr_filename\n",
        "            data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # STEP 3: Load the input image.\n",
        "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "            # STEP 4: Detect hand landmarks from the input image.\n",
        "            detection_result = detector.detect(image)\n",
        "            # STEP 5: Process the classification result. In this case, visualize it.\n",
        "            annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "            cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "            printed_frames += 1\n",
        "        curr_frame += 1\n",
        "\n",
        "# details about the landmarks https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker"
      ],
      "metadata": {
        "id": "fERlz_12G7q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hand detection sample\n",
        "\n",
        "sample = training_video + \"adjective_01066.mp4\"\n",
        "i, j = 1603, 3\n",
        "cap = cv2.VideoCapture(sample)\n",
        "frame_end = js_file['instances'][int(i)][int(j)]['frame_end']\n",
        "frame_start = js_file['instances'][int(i)][int(j)]['frame_start']\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_end = frame_end if frame_end !=-1 else total_frames\n",
        "step_frame = (frame_end - frame_start + 1) // 16\n",
        "curr_frame = 0\n",
        "printed_frames = 0\n",
        "\n",
        "# STEP 2: Create an HandLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                       num_hands=2)\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "  if curr_frame < frame_start:\n",
        "    curr_frame += 1\n",
        "  if frame_end != -1 and curr_frame > frame_end:\n",
        "    break\n",
        "  if (curr_frame - frame_start) % step_frame == 0 and printed_frames < 32:\n",
        "    data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # STEP 3: Load the input image.\n",
        "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=data)\n",
        "    # STEP 4: Detect hand landmarks from the input image.\n",
        "    detection_result = detector.detect(image)\n",
        "    # STEP 5: Process the classification result. In this case, visualize it.\n",
        "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "    landmarks = detection_result.hand_landmarks\n",
        "    if landmarks != []:\n",
        "      # print only images in which the hand(s) has(ve) been detected\n",
        "      cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "      for idx, hand_landmarks in enumerate(landmarks):\n",
        "        x = [landmark.x for landmark in hand_landmarks] # Normalized x coordinate [0.0, 1.0]\n",
        "        y = [landmark.y for landmark in hand_landmarks] # Normalized x coordinate [0.0, 1.0]\n",
        "        z = [landmark.z for landmark in hand_landmarks] # Relative depth\n",
        "        visibility = [landmark.visibility for landmark in hand_landmarks] # Confidence score of the landmark visibility\n",
        "\n",
        "    printed_frames += 1\n",
        "  curr_frame += 1"
      ],
      "metadata": {
        "id": "mvSNuaqFSouJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}